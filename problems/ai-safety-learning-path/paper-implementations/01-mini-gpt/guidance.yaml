title: "Mini-GPT from Scratch"
pattern: "Transformer Architecture"
difficulty: "hard"

why_this_matters: |
  You can't do AI safety work without understanding how LLMs work.
  Building a transformer from scratch gives you:
  - Intuition for how attention creates contextual understanding
  - Understanding of the inductive biases in the architecture
  - Foundation for interpretability and alignment work
  - Practical PyTorch skills for implementing research papers

hints:
  key_concepts:
    - text: |
        ATTENTION is the core innovation:
        
        Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
        
        - Q (Query): "What am I looking for?"
        - K (Key): "What do I contain?"
        - V (Value): "What do I offer?"
        
        High QÂ·K similarity â†’ high attention weight â†’ more of that V in output
    
    - text: |
        MULTI-HEAD attention lets the model attend to different things:
        
        Head 1 might track syntax
        Head 2 might track semantics
        Head 3 might track position
        
        Split d_model into n_heads, run attention in parallel, concatenate.
    
    - text: |
        CAUSAL MASKING prevents cheating during training:
        
        Position i can only attend to positions 0, 1, ..., i
        Not i+1, i+2, ... (the future!)
        
        Implemented by setting future positions to -inf before softmax.
    
    - text: |
        RESIDUAL CONNECTIONS are crucial for deep networks:
        
        x = x + Attention(LayerNorm(x))
        x = x + FFN(LayerNorm(x))
        
        Gradients flow directly through the residual stream,
        preventing vanishing gradients in deep models.

  common_mistakes:
    - text: |
        âš ï¸ Forgetting to scale by âˆšd_k:
        Without scaling, softmax saturates with large d_k.
        Gradients become tiny â†’ training fails.
    
    - text: |
        âš ï¸ Wrong mask dimension or orientation:
        Mask should be (seq, seq), with 0s for "don't attend".
        Lower triangular for causal (can attend to past, not future).
    
    - text: |
        âš ï¸ Not using causal mask during generation:
        Even though you're generating one token at a time,
        the mask is needed for consistent behavior.
    
    - text: |
        âš ï¸ Forgetting dropout in attention weights:
        Add dropout AFTER softmax, BEFORE multiplying by V.

  real_world:
    - text: |
        ðŸ¤– GPT-2 / GPT-3 / GPT-4
        All use this exact architecture (scaled up).
        GPT-3: 175B parameters, 96 layers, 96 heads, d_model=12288
        
        ðŸ¤– Claude
        Uses transformer architecture with safety-focused training.
        
        ðŸ”¬ Why Decoder-Only?
        GPT (decoder-only) vs BERT (encoder-only) vs T5 (encoder-decoder)
        
        Decoder-only is simpler and scales better for generation tasks.
        Causal attention enables efficient KV caching during inference.

  solution_approach:
    overview: |
      Build bottom-up: embeddings â†’ attention â†’ blocks â†’ full model
    
    steps:
      - "1. Token embedding: nn.Embedding(vocab_size, d_model)"
      - "2. Position embedding: nn.Embedding(max_seq_len, d_model) or sinusoidal"
      - "3. Multi-head attention: Project to Q,K,V, reshape for heads, compute attention, concatenate"
      - "4. FFN: Linear(d_model, d_ff) â†’ GELU â†’ Linear(d_ff, d_model)"
      - "5. Transformer block: LayerNorm â†’ Attention + residual â†’ LayerNorm â†’ FFN + residual"
      - "6. Full model: Embeddings â†’ N blocks â†’ LayerNorm â†’ Linear(d_model, vocab_size)"
      - "7. Training: Cross-entropy loss, AdamW, gradient clipping, learning rate warmup"
      - "8. Generation: Autoregressive loop with temperature/top-k/top-p sampling"

complexity:
  time: "O(nÂ² Ã— d) per layer"
  space: "O(nÂ² Ã— h) for attention matrices"

debugging_tips:
  - symptom: "NaN loss"
    likely_cause: "Missing scaling in attention, or numerical overflow"
    fix: "Add scaling by sqrt(d_k), check for exp() overflow in softmax"
  
  - symptom: "Model outputs same token repeatedly"
    likely_cause: "Temperature too low or broken sampling"
    fix: "Check temperature is > 0, verify top-k/top-p logic"
  
  - symptom: "Loss doesn't decrease"
    likely_cause: "Learning rate too high/low, or architecture bug"
    fix: "Try lr=3e-4 (AdamW default), verify shapes at each step"

next_steps:
  - "Add FlashAttention for efficiency"
  - "Implement KV caching for faster inference"
  - "Try Rotary Position Embeddings (RoPE)"
  - "Scale up: More layers, bigger d_model"
