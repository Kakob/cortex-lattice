id: "matrix-multiplication-attention"
title: "Matrix Multiplication - Attention Weight Computation"
difficulty: "medium"
pattern: "Matrix Operations"
tags: ["linear-algebra", "transformers", "attention", "numpy"]

description: |
  ðŸš€ MISSION: Compute Attention Alignment Matrices
  
  In the heart of every transformer model lies matrix multiplication.
  Your spacecraft's AI navigation system uses attention mechanisms to
  focus on relevant sensor data. Implement efficient matrix multiplication
  to compute attention weights between query and key matrices.
  
  Given two matrices Q (queries) and K (keys), compute the attention
  alignment scores: A = Q @ K^T (Q multiplied by K transposed).
  
  This is the foundation of the attention mechanism in transformers!

examples:
  - input:
      Q: [[1, 0], [0, 1]]
      K: [[1, 2], [3, 4]]
    output: [[1, 3], [2, 4]]
    explanation: |
      Q @ K^T where K^T = [[1, 3], [2, 4]]
      Row 0: [1,0] @ [[1,3],[2,4]] = [1*1+0*2, 1*3+0*4] = [1, 3]
      Row 1: [0,1] @ [[1,3],[2,4]] = [0*1+1*2, 0*3+1*4] = [2, 4]
  
  - input:
      Q: [[1, 2, 3]]
      K: [[4, 5, 6], [7, 8, 9]]
    output: [[32, 50]]
    explanation: |
      Q (1x3) @ K^T (3x2) = (1x2)
      [1,2,3] @ [4,7] = 1*4 + 2*5 + 3*6 = 32
      [1,2,3] @ [5,8] = 1*7 + 2*8 + 3*9 = 50

constraints:
  - "1 <= Q.rows, Q.cols, K.rows, K.cols <= 512"
  - "Q.cols == K.cols (dimension must match for Q @ K^T)"
  - "All values are floats in range [-100, 100]"

starter_code:
  python: |
    from typing import List
    
    def attention_alignment(Q: List[List[float]], K: List[List[float]]) -> List[List[float]]:
        """
        Compute attention alignment scores: A = Q @ K^T
        
        Args:
            Q: Query matrix of shape (seq_len_q, d_k)
            K: Key matrix of shape (seq_len_k, d_k)
        
        Returns:
            Attention alignment matrix of shape (seq_len_q, seq_len_k)
        """
        # Your code here
        pass

test_cases:
  - input:
      Q: [[1, 0], [0, 1]]
      K: [[1, 2], [3, 4]]
    output: [[1, 3], [2, 4]]
  
  - input:
      Q: [[1, 2, 3]]
      K: [[4, 5, 6], [7, 8, 9]]
    output: [[32, 50]]
  
  - input:
      Q: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
      K: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
    output: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
  
  - input:
      Q: [[2, 3], [4, 5], [6, 7]]
      K: [[1, 1]]
    output: [[5], [9], [13]]
  
  - input:
      Q: [[0.5, -0.5], [-0.5, 0.5]]
      K: [[1, 1], [-1, -1]]
    output: [[0, 0], [0, 0]]

complexity:
  time: "O(n * m * k) where Q is (n x k) and K is (m x k)"
  space: "O(n * m) for the output matrix"

related_problems:
  - "Scaled Dot-Product Attention"
  - "Multi-Head Attention"
  - "Batch Matrix Multiplication"

papers:
  - title: "Attention Is All You Need"
    authors: "Vaswani et al."
    year: 2017
    relevance: "This operation is the core of the attention mechanism"
