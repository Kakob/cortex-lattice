title: "Layer Normalization"
pattern: "Normalization"
difficulty: "medium"

why_this_matters: |
  Layer normalization is in EVERY transformer layer!
  It keeps activations in a stable range, enabling deep networks.

hints:
  key_concepts:
    - text: |
        Layer Norm normalizes across features (last dimension).
        Each sample is normalized independently.
        
        BatchNorm normalizes across batch (problematic for NLP).
        LayerNorm works with any batch size, even batch_size=1.
    
    - text: |
        The epsilon (Œµ ‚âà 1e-5) prevents division by zero
        when variance is very small.
    
    - text: |
        Œ≥ (gamma) and Œ≤ (beta) are learned parameters.
        - Œ≥: scales the normalized output
        - Œ≤: shifts the normalized output
        
        This allows the network to "undo" normalization if needed.

  common_mistakes:
    - text: |
        ‚ö†Ô∏è Normalizing over wrong dimension:
        LayerNorm: normalize over FEATURES (last dim)
        BatchNorm: normalize over BATCH (first dim)
    
    - text: |
        ‚ö†Ô∏è Forgetting epsilon in sqrt:
        ‚àö(œÉ¬≤ + Œµ), NOT ‚àöœÉ¬≤
        Zero variance ‚Üí division by zero!

  real_world:
    - text: |
        ü§ñ Transformer Architecture
        - Pre-LN: LayerNorm before attention/FFN (GPT-2)
        - Post-LN: LayerNorm after attention/FFN (original)
        
        Pre-LN is more stable for training deep networks.

  solution_approach:
    steps:
      - "1. For each row in x:"
      - "2.   Compute mean: Œº = mean(row)"
      - "3.   Compute variance: œÉ¬≤ = mean((row - Œº)¬≤)"
      - "4.   Normalize: normalized = (row - Œº) / ‚àö(œÉ¬≤ + Œµ)"
      - "5.   Scale and shift: output = Œ≥ √ó normalized + Œ≤"

complexity:
  time: "O(n √ó d)"
  space: "O(n √ó d)"
