id: "circular-buffer-replay"
title: "Experience Replay Buffer - Memory for Reinforcement Learning"
difficulty: "medium"
pattern: "Circular Buffer"
tags: ["reinforcement-learning", "data-structures", "ppo", "dqn"]

description: |
  ðŸš€ MISSION: Build Memory for Your RL Agent
  
  Your spacecraft's autopilot learns from experience. But it can't store
  infinite memories! Implement a circular buffer that:
  
  1. Stores up to N experiences (transitions)
  2. Overwrites oldest when full (FIFO)
  3. Supports random sampling for training
  
  This is the CORE data structure for DQN, PPO, and all off-policy RL!

examples:
  - input:
      capacity: 3
      operations:
        - push: {state: [1], action: 0, reward: 1, next_state: [2]}
        - push: {state: [2], action: 1, reward: 2, next_state: [3]}
        - push: {state: [3], action: 0, reward: 3, next_state: [4]}
        - push: {state: [4], action: 1, reward: 4, next_state: [5]}  # Overwrites first!
        - sample: 2
    output: "Buffer contains last 3 experiences, sample returns 2 random ones"

constraints:
  - "1 <= capacity <= 1,000,000"
  - "Experiences are (state, action, reward, next_state, done) tuples"

starter_code:
  python: |
    from typing import List, Tuple, Any
    import random
    
    class ReplayBuffer:
        def __init__(self, capacity: int):
            """Initialize buffer with given capacity."""
            # Your code here
            pass
        
        def push(self, state, action, reward, next_state, done):
            """Add experience to buffer."""
            # Your code here
            pass
        
        def sample(self, batch_size: int) -> List[Tuple]:
            """Sample random batch of experiences."""
            # Your code here
            pass
        
        def __len__(self) -> int:
            """Return current number of experiences."""
            # Your code here
            pass

complexity:
  time: "O(1) push, O(batch_size) sample"
  space: "O(capacity)"
